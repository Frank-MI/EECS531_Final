{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Author: Yu Mi, yxm319; Boning Zhao, bxz213\n",
    "Recognizing human actions is one of most popular computer vision method which finds mutiple applications in lots of fields such as video surveillance, customer attributes, shopping behavior analysis.\n",
    "\n",
    "In our final project, we consider the automated recognition of human actions in some videos. We proposed to build up a 3D CNN model for action recognition. In order to capture motion information from multiple adjacent frames, we proposed to extract features from both spatial and temporal dimensions. Based on this feature extractor, a 3D convolutional neural network will be built up. This CNN will generates multiple channels of information and performs convolution and subsampling separately. The final feature representation is obtained by conbining information from all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices for trainning: ['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Import standard and supportive libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Available devices for trainning:\", get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nerual network framework\n",
    "In this project, we are going to apply [Keras](https://keras.io) as our neural network framework since it is already introduced in Homework3. It is capable of running on top of TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay and good for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use tensorflow in GPU\n",
    "# config_tf = tf.ConfigProto(log_device_placement=True)\n",
    "# config_tf.gpu_options.allow_growth = True\n",
    "# session = tf.Session(config=config_tf)\n",
    "\n",
    "# Import models and layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "\n",
    "# Import utilities\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.backend import set_session\n",
    "# set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KTH dataset\n",
    "[KTH dataset](http://www.nada.kth.se/cvap/actions/) is a database provided by KTH Royal institute of Technology. The current video database contains six tyes of human actions, including walking, jogging, running, boxing, hand waving and hand clapping. All the actions are performed several times by 25 different individuals in for scenarios: outdoors $s1$, outdoors with scale variation $s2$, outdoors with different clothes $s3$ and indoors $s4$ as illustrated below. \n",
    "![KTH scenarios and actions](figure/KTH_Intro.gif)\n",
    "Currently we have $600$ sequences in the dataset and all the sequences were taken over homogeneous backgrounds with a static camera with $25$fps frame rate. The sequences were downsampled to the spatial resolution of $160\\times120$ pixels and have a length of four seconds in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxing class successfully loaded.\n",
      "Hand-clapping class successfully loaded.\n",
      "Hand-waving class successfully loaded.\n",
      "Jogging class successfully loaded.\n",
      "Running class successfully loaded.\n",
      "Walking class successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# image attributes\n",
    "img_r, img_c, img_d = 34, 54, 9\n",
    "#img_r, img_c, img_d = 15, 15, 16\n",
    "#Training set\n",
    "#Entire dataset\n",
    "Training_set=[]\n",
    "#Loading boxing class\n",
    "box_listing = os.listdir('data/kth_database/boxing')\n",
    "for box_id in box_listing:\n",
    "    box_id = 'data/kth_database/boxing/'+box_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(box_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Boxing class successfully loaded.\")  \n",
    "\n",
    "#Loading hand clapping class\n",
    "hc_listing = os.listdir('data/kth_database/handclapping')\n",
    "for hc_id in hc_listing:\n",
    "    hc_id = 'data/kth_database/handclapping/'+hc_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hc_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand-clapping class successfully loaded.\")\n",
    "\n",
    "#Loading hand waving class\n",
    "hw_listing = os.listdir('data/kth_database/handwaving')\n",
    "for hw_id in hw_listing:\n",
    "    hw_id = 'data/kth_database/handwaving/'+hw_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hw_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand-waving class successfully loaded.\")\n",
    "\n",
    "#Loading jogging class\n",
    "jog_listing = os.listdir('data/kth_database/jogging')\n",
    "for jog_id in jog_listing:\n",
    "    jog_id = 'data/kth_database/jogging/'+jog_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(jog_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Jogging class successfully loaded.\")\n",
    "\n",
    "#Loading running class\n",
    "run_listing = os.listdir('data/kth_database/running')\n",
    "for run_id in run_listing:\n",
    "    run_id = 'data/kth_database/running/'+run_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(run_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Running class successfully loaded.\")\n",
    "\n",
    "#Loading walking class\n",
    "walk_listing = os.listdir('data/kth_database/walking')\n",
    "for walk_id in walk_listing:\n",
    "    walk_id = 'data/kth_database/walking/'+walk_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(walk_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Walking class successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:99: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "#convert the fram into array\n",
    "Training_data=np.array(Training_set)\n",
    "sample_num = len(Training_data)\n",
    "#Assign Label\n",
    "label = np.ones((sample_num,),dtype = int)\n",
    "label[0:100] = 0\n",
    "label[100:199] = 1\n",
    "label[199:299] = 2\n",
    "label[299:399] = 3\n",
    "label[399:499] = 4\n",
    "label[499:] = 5\n",
    "#print(Training_data.shape)\n",
    "#print(label.shape)\n",
    "train= [Training_data,label]\n",
    "train_set = np.zeros((sample_num, img_r,img_c,img_d,1))\n",
    "\n",
    "for i in range(sample_num):\n",
    "    for j in range(img_r):\n",
    "        for k in range(img_c):\n",
    "            for l in range(img_d):\n",
    "                train_set[i][j][k][l][0]=train[0][i,j,k,l]\n",
    "                #print(i)\n",
    "\n",
    "#training parameter for CNN\n",
    "classes = 6\n",
    "epoch =20\n",
    "batch_size = 2\n",
    "#number of frames\n",
    "patch_size = 15\n",
    "\n",
    "\n",
    "(X_train, y_train) = (train[0],train[1])\n",
    "Y_train = np_utils.to_categorical(y_train, classes)\n",
    "\n",
    "#number of convoluntional filters\n",
    "filt =[32, # 1st layer \n",
    "       32  # 2nd layer\n",
    "      ]\n",
    "#level of pooling \n",
    "pool = [3,3]\n",
    "#level of convolution\n",
    "conv = [5,5]\n",
    "\n",
    "#preprocessing part\n",
    "train_set = train_set.astype('float32')\n",
    "train_set -= np.mean(train_set)\n",
    "train_set /= np.max(train_set)\n",
    "\n",
    "#Building the CNN model\n",
    "with tf.device('/device:CPU:0'):\n",
    "    model = Sequential()\n",
    "    '''\n",
    "    model.add(Conv3D(\n",
    "            filters=7,\n",
    "            kernel_size = (28,48,5),\n",
    "            strides=(1, 1, 1),\n",
    "            activation='relu',\n",
    "            input_shape = (34,54,9,7)\n",
    "            ))\n",
    "\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(14, 24, 5)))\n",
    "    print('Test')\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Conv3D(\n",
    "            filters=35,\n",
    "            kernel_size = (10,20,3),\n",
    "            strides=(1, 1, 1),\n",
    "            activation='relu',\n",
    "            ))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(5, 10, 3)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Conv3D(\n",
    "            filters=5,\n",
    "            kernel_size = (3,8,1),\n",
    "            strides=(1, 1, 1),\n",
    "            activation='relu',\n",
    "            ))\n",
    "    model.add(Dense(30,init='normal'))\n",
    "    model.add(Dense(6,init='normal'))\n",
    "    '''\n",
    "    model.add(Conv3D(\n",
    "            filters=filt[0],\n",
    "            kernel_size = (5,5,5),\n",
    "            input_shape=(img_r, img_c, img_d,1),\n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "    print('Test')\n",
    "    model.add(MaxPooling3D(pool_size=(pool[0], pool[0], pool[0])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes,init='normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['mse', 'accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 6)\n",
      "(479, 34, 54, 9, 1)\n",
      "(479, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 479 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "479/479 [==============================] - 207s 433ms/step - loss: 1.7015 - mean_squared_error: 0.1361 - acc: 0.2818 - val_loss: 1.3589 - val_mean_squared_error: 0.1161 - val_acc: 0.4333\n",
      "Epoch 2/20\n",
      "479/479 [==============================] - 193s 404ms/step - loss: 1.4310 - mean_squared_error: 0.1234 - acc: 0.3674 - val_loss: 1.2520 - val_mean_squared_error: 0.1135 - val_acc: 0.4750\n",
      "Epoch 3/20\n",
      "479/479 [==============================] - 192s 402ms/step - loss: 1.3611 - mean_squared_error: 0.1219 - acc: 0.3987 - val_loss: 1.1433 - val_mean_squared_error: 0.1122 - val_acc: 0.4333\n",
      "Epoch 4/20\n",
      "479/479 [==============================] - 192s 401ms/step - loss: 1.2313 - mean_squared_error: 0.1135 - acc: 0.4134 - val_loss: 0.9864 - val_mean_squared_error: 0.0961 - val_acc: 0.4667\n",
      "Epoch 5/20\n",
      "479/479 [==============================] - 192s 401ms/step - loss: 1.2082 - mean_squared_error: 0.1098 - acc: 0.4781 - val_loss: 1.0407 - val_mean_squared_error: 0.1029 - val_acc: 0.4417\n",
      "Epoch 6/20\n",
      "479/479 [==============================] - 192s 400ms/step - loss: 1.1371 - mean_squared_error: 0.1052 - acc: 0.4969 - val_loss: 0.9655 - val_mean_squared_error: 0.0961 - val_acc: 0.5250\n",
      "Epoch 7/20\n",
      "479/479 [==============================] - 192s 401ms/step - loss: 1.1323 - mean_squared_error: 0.1066 - acc: 0.4760 - val_loss: 1.1644 - val_mean_squared_error: 0.0988 - val_acc: 0.5250\n",
      "Epoch 8/20\n",
      "479/479 [==============================] - 190s 396ms/step - loss: 1.1475 - mean_squared_error: 0.1027 - acc: 0.5157 - val_loss: 0.9096 - val_mean_squared_error: 0.0906 - val_acc: 0.5500\n",
      "Epoch 9/20\n",
      "479/479 [==============================] - 191s 399ms/step - loss: 1.0424 - mean_squared_error: 0.0951 - acc: 0.5553 - val_loss: 1.0622 - val_mean_squared_error: 0.1044 - val_acc: 0.4833\n",
      "Epoch 10/20\n",
      "479/479 [==============================] - 189s 395ms/step - loss: 1.0444 - mean_squared_error: 0.0935 - acc: 0.5658 - val_loss: 0.8050 - val_mean_squared_error: 0.0768 - val_acc: 0.6167\n",
      "Epoch 11/20\n",
      "479/479 [==============================] - 189s 395ms/step - loss: 1.0120 - mean_squared_error: 0.0912 - acc: 0.5846 - val_loss: 0.8622 - val_mean_squared_error: 0.0862 - val_acc: 0.5250\n",
      "Epoch 12/20\n",
      "479/479 [==============================] - 190s 396ms/step - loss: 1.0035 - mean_squared_error: 0.0862 - acc: 0.6180 - val_loss: 0.8949 - val_mean_squared_error: 0.0833 - val_acc: 0.6167\n",
      "Epoch 13/20\n",
      "479/479 [==============================] - 189s 394ms/step - loss: 0.8944 - mean_squared_error: 0.0824 - acc: 0.6054 - val_loss: 0.9606 - val_mean_squared_error: 0.0833 - val_acc: 0.5833\n",
      "Epoch 14/20\n",
      "479/479 [==============================] - 190s 396ms/step - loss: 0.8897 - mean_squared_error: 0.0798 - acc: 0.6388 - val_loss: 1.3413 - val_mean_squared_error: 0.0909 - val_acc: 0.5750\n",
      "Epoch 15/20\n",
      "479/479 [==============================] - 189s 394ms/step - loss: 0.9353 - mean_squared_error: 0.0835 - acc: 0.6075 - val_loss: 1.2594 - val_mean_squared_error: 0.0940 - val_acc: 0.5583\n",
      "Epoch 16/20\n",
      "479/479 [==============================] - 189s 394ms/step - loss: 0.8696 - mean_squared_error: 0.0763 - acc: 0.6639 - val_loss: 1.0978 - val_mean_squared_error: 0.0856 - val_acc: 0.5917\n",
      "Epoch 17/20\n",
      "479/479 [==============================] - 189s 394ms/step - loss: 0.9412 - mean_squared_error: 0.0755 - acc: 0.6681 - val_loss: 1.0178 - val_mean_squared_error: 0.0897 - val_acc: 0.5667\n",
      "Epoch 18/20\n",
      "479/479 [==============================] - 190s 397ms/step - loss: 0.8697 - mean_squared_error: 0.0763 - acc: 0.6576 - val_loss: 1.0036 - val_mean_squared_error: 0.0846 - val_acc: 0.5750\n",
      "Epoch 19/20\n",
      "479/479 [==============================] - 206s 430ms/step - loss: 0.8754 - mean_squared_error: 0.0752 - acc: 0.6576 - val_loss: 1.1287 - val_mean_squared_error: 0.0837 - val_acc: 0.6417\n",
      "Epoch 20/20\n",
      "479/479 [==============================] - 208s 433ms/step - loss: 0.8675 - mean_squared_error: 0.0766 - acc: 0.6451 - val_loss: 1.0227 - val_mean_squared_error: 0.0841 - val_acc: 0.6000\n",
      "120/120 [==============================] - 2s 14ms/step\n",
      "['loss', 'mean_squared_error', 'acc']\n",
      "Test score: [1.022670391226893, 0.08408391158873625, 0.6]\n",
      "History {'val_loss': [1.358925857146581, 1.2519663641850154, 1.14334021260341, 0.9863539129495621, 1.040739301343759, 0.9655379203458627, 1.1644467432051897, 0.9096464727073907, 1.062185008994614, 0.8050011440180243, 0.8621762432313214, 0.8949235039918373, 0.9606209197856515, 1.3413443748089775, 1.2594250000392397, 1.0978228847724192, 1.017765080514664, 1.003603293590172, 1.1286763893958218, 1.022670391226893], 'val_mean_squared_error': [0.1161227285861969, 0.11348777301609517, 0.11221446289370457, 0.09605022664181888, 0.10285661650511126, 0.09613125490335127, 0.0987543786643073, 0.09057419097516686, 0.10441491636871282, 0.07675312350541692, 0.08619018237368437, 0.08333674593617009, 0.08327509144494853, 0.09089827046691244, 0.09395033763765308, 0.08556207752492515, 0.0896771463759661, 0.08458230676962035, 0.08368951729718198, 0.08408391158873625], 'val_acc': [0.43333333333333335, 0.475, 0.43333333333333335, 0.4666666666666667, 0.44166666666666665, 0.525, 0.525, 0.55, 0.48333333333333334, 0.6166666666666667, 0.525, 0.6166666666666667, 0.5833333333333334, 0.575, 0.5583333333333333, 0.5916666666666667, 0.5666666666666667, 0.575, 0.6416666666666667, 0.6], 'loss': [1.7015443136896122, 1.4309718272880123, 1.3611349812193554, 1.2313228688807478, 1.2081645788131028, 1.1370898672148182, 1.1323181802585782, 1.147477689329418, 1.042361835332778, 1.0444140093938674, 1.0120224200222168, 1.0035096877548622, 0.8943753302802617, 0.8897309621282429, 0.9352571464886236, 0.8696043449824643, 0.9412097443076511, 0.8697106053550365, 0.8754048000645953, 0.8674569974053912], 'mean_squared_error': [0.1360930951938251, 0.12338434774934126, 0.12194453396626392, 0.11350939495368455, 0.10983262860871432, 0.1052486133355344, 0.10662693316348727, 0.10268856891983097, 0.09508081177582027, 0.09353502011820083, 0.09119487310990834, 0.0861556772376465, 0.08236913565728737, 0.07976016335533218, 0.08349539686608566, 0.07630428700072273, 0.07547273159227566, 0.07627461325007268, 0.07518162889985307, 0.0765515046114478], 'acc': [0.2818371607515658, 0.3674321503131524, 0.3987473903966597, 0.4133611691022965, 0.4780793319415449, 0.4968684759916493, 0.4759916492693111, 0.5156576200417536, 0.5553235908141962, 0.5657620041753654, 0.5845511482254697, 0.6179540709812108, 0.605427974947808, 0.6388308977035491, 0.6075156576200418, 0.6638830897703549, 0.6680584551148225, 0.6576200417536534, 0.6576200417536534, 0.6450939457202505]}\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    print(Y_train.shape)\n",
    "    #Split the data for Train and Test\n",
    "    X_train_new, X_val_new, y_train_new,y_val_new = train_test_split(train_set, Y_train, test_size=0.2, random_state=4)\n",
    "    print(X_train_new.shape)\n",
    "    print(y_train_new.shape)\n",
    "    #Training\n",
    "    hist = model.fit(\n",
    "        X_train_new,\n",
    "        y_train_new,\n",
    "        validation_data=(X_val_new,y_val_new),\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch = epoch,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    #Testing\n",
    "    score = model.evaluate(\n",
    "        X_val_new,\n",
    "        y_val_new,\n",
    "        batch_size=batch_size,\n",
    "        #show_accuracy=True\n",
    "        )\n",
    "    print(model.metrics_names);\n",
    "\n",
    "    print('Test score:', score)\n",
    "\n",
    "    print('History', hist.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rollaxis(ipt,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rollaxis(np.rollaxis(ipt,2,0),2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
