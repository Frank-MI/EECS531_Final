{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Author: Yu Mi, yxm319; Boning Zhao, bxz213\n",
    "Recognizing human actions is one of most popular computer vision method which finds mutiple applications in lots of fields such as video surveillance, customer attributes, shopping behavior analysis.\n",
    "\n",
    "In our final project, we consider the automated recognition of human actions in some videos. We proposed to build up a 3D CNN model for action recognition. In order to capture motion information from multiple adjacent frames, we proposed to extract features from both spatial and temporal dimensions. Based on this feature extractor, a 3D convolutional neural network will be built up. This CNN will generates multiple channels of information and performs convolution and subsampling separately. The final feature representation is obtained by conbining information from all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard and supportive libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nerual network framework\n",
    "In this project, we are going to apply [Keras](https://keras.io) as our neural network framework since it is already introduced in Homework3. It is capable of running on top of TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay and good for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "\n",
    "# Import utilities\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils, generic_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KTH dataset\n",
    "[KTH dataset](http://www.nada.kth.se/cvap/actions/) is a database provided by KTH Royal institute of Technology. The current video database contains six tyes of human actions, including walking, jogging, running, boxing, hand waving and hand clapping. All the actions are performed several times by 25 different individuals in for scenarios: outdoors $s1$, outdoors with scale variation $s2$, outdoors with different clothes $s3$ and indoors $s4$ as illustrated below. \n",
    "![KTH scenarios and actions](figure/KTH_Intro.gif)\n",
    "Currently we have $600$ sequences in the dataset and all the sequences were taken over homogeneous backgrounds with a static camera with $25$fps frame rate. The sequences were downsampled to the spatial resolution of $160\\times120$ pixels and have a length of four seconds in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image attributes\n",
    "img_r, img_c, img_d = 16, 16, 15\n",
    "#Training set\n",
    "#Entire dataset\n",
    "Training_set=[]\n",
    "#Loading boxing class\n",
    "box_listing = os.listdir('data/kth_database/boxing')\n",
    "for box_id in box_listing:\n",
    "    box_id = 'data/kth_database/boxing/'+box_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(box_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Boxing class has been loaded\")  \n",
    "\n",
    "#Loading hand clapping class\n",
    "hc_listing = os.listdir('data/kth_database/handclapping')\n",
    "for hc_id in hc_listing:\n",
    "    hc_id = 'data/kth_database/handclapping/'+hc_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hc_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand clapping class has been loaded\")\n",
    "\n",
    "#Loading hand waving class\n",
    "hw_listing = os.listdir('data/kth_database/handwaving')\n",
    "for hw_id in hw_listing:\n",
    "    hw_id = 'data/kth_database/handwaving/'+hc_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hw_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand waving class has been loaded\")\n",
    "\n",
    "#Loading jogging class\n",
    "jog_listing = os.listdir('data/kth_database/jogging')\n",
    "for jog_id in jog_listing:\n",
    "    jog_id = 'data/kth_database/jogging/'+jog_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(jog_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Jogging class has been loaded\")\n",
    "\n",
    "#Loading running class\n",
    "run_listing = os.listdir('data/kth_database/running')\n",
    "for run_id in run_listing:\n",
    "    run_id = 'data/kth_database/running/'+run_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(run_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"running class has been loaded\")\n",
    "\n",
    "#Loading walking class\n",
    "walk_listing = os.listdir('data/kth_database/walking')\n",
    "for walk_id in walk_listing:\n",
    "    walk_id = 'data/kth_database/walking/'+walk_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(walk_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    #ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,0)\n",
    "    Training_set.append(ipt)\n",
    "print(\"walking class has been loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
