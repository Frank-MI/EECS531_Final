{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Author: Yu Mi, yxm319; Boning Zhao, bxz213\n",
    "Recognizing human actions is one of most popular computer vision method which finds mutiple applications in lots of fields such as video surveillance, customer attributes, shopping behavior analysis.\n",
    "\n",
    "In our final project, we consider the automated recognition of human actions in some videos. We proposed to build up a 3D CNN model for action recognition. In order to capture motion information from multiple adjacent frames, we proposed to extract features from both spatial and temporal dimensions. Based on this feature extractor, a 3D convolutional neural network will be built up. This CNN will generates multiple channels of information and performs convolution and subsampling separately. The final feature representation is obtained by conbining information from all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices for trainning: ['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Import standard and supportive libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Available devices for trainning:\", get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nerual network framework\n",
    "In this project, we are going to apply [Keras](https://keras.io) as our neural network framework since it is already introduced in Homework3. It is capable of running on top of TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay and good for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Try to use tensorflow in GPU\n",
    "config_tf = tf.ConfigProto(log_device_placement=True)\n",
    "config_tf.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config_tf)\n",
    "\n",
    "# Import models and layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "\n",
    "# Import utilities\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.backend import set_session\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KTH dataset\n",
    "[KTH dataset](http://www.nada.kth.se/cvap/actions/) is a database provided by KTH Royal institute of Technology. The current video database contains six tyes of human actions, including walking, jogging, running, boxing, hand waving and hand clapping. All the actions are performed several times by 25 different individuals in for scenarios: outdoors $s1$, outdoors with scale variation $s2$, outdoors with different clothes $s3$ and indoors $s4$ as illustrated below. \n",
    "![KTH scenarios and actions](figure/KTH_Intro.gif)\n",
    "Currently we have $600$ sequences in the dataset and all the sequences were taken over homogeneous backgrounds with a static camera with $25$fps frame rate. The sequences were downsampled to the spatial resolution of $160\\times120$ pixels and have a length of four seconds in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the KTH data as input\n",
    "In loading the KTH dataset, we try to import every second of the frames (including $24$ frames) as a sequence, since each video file have much more than $1$ second, we select the first $4$ seconds of the frames to be the input, which means we are going to extract nearly $2400$ sequences as our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxing class successfully loaded.\n",
      "Hand-clapping class successfully loaded.\n",
      "Hand-waving class successfully loaded.\n",
      "Jogging class successfully loaded.\n",
      "Running class successfully loaded.\n",
      "Walking class successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "inflation = 4\n",
    "# image attributes\n",
    "img_r, img_c, img_d = 64, 48, 24\n",
    "#img_r, img_c, img_d = 15, 15, 16\n",
    "#Training set\n",
    "#Entire dataset\n",
    "Training_set=[]\n",
    "#Loading boxing class\n",
    "box_listing = os.listdir('data/kth_database/boxing')\n",
    "for box_id in box_listing:\n",
    "    box_id = 'data/kth_database/boxing/'+box_id\n",
    "    capture = cv2.VideoCapture(box_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "        #print(ipt.shape)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Boxing class successfully loaded.\")  \n",
    "\n",
    "#Loading hand clapping class\n",
    "hc_listing = os.listdir('data/kth_database/handclapping')\n",
    "for hc_id in hc_listing:\n",
    "    hc_id = 'data/kth_database/handclapping/'+hc_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hc_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Hand-clapping class successfully loaded.\")\n",
    "\n",
    "#Loading hand waving class\n",
    "hw_listing = os.listdir('data/kth_database/handwaving')\n",
    "for hw_id in hw_listing:\n",
    "    hw_id = 'data/kth_database/handwaving/'+hw_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hw_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Hand-waving class successfully loaded.\")\n",
    "\n",
    "#Loading jogging class\n",
    "jog_listing = os.listdir('data/kth_database/jogging')\n",
    "for jog_id in jog_listing:\n",
    "    jog_id = 'data/kth_database/jogging/'+jog_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(jog_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Jogging class successfully loaded.\")\n",
    "\n",
    "#Loading running class\n",
    "run_listing = os.listdir('data/kth_database/running')\n",
    "for run_id in run_listing:\n",
    "    run_id = 'data/kth_database/running/'+run_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(run_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Running class successfully loaded.\")\n",
    "\n",
    "#Loading walking class\n",
    "walk_listing = os.listdir('data/kth_database/walking')\n",
    "for walk_id in walk_listing:\n",
    "    walk_id = 'data/kth_database/walking/'+walk_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(walk_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for j in range(inflation):\n",
    "        frame_list = []\n",
    "        for i in range(img_d):\n",
    "            success, frame = capture.read()\n",
    "            frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "            gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_list.append(gray)\n",
    "        cv2.destroyAllWindows()\n",
    "        ipt = np.asarray(frame_list)\n",
    "        ipt = np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    #     frame_length = len(ipt)\n",
    "    #     fourth = int(frame_length/4)\n",
    "    #     for i in range(0,frame_length,fourth):\n",
    "    #         Training_set.append(ipt[i:i+fourth])\n",
    "        Training_set.append(ipt)\n",
    "    capture.release()\n",
    "print(\"Walking class successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the frames into array\n",
    "#print(len(Training_set),len(Training_set[0]),len(Training_set[0][0]),len(Training_set[0][0][0]))\n",
    "for i in range(len(Training_set)):\n",
    "    assert(len(Training_set[i])==64),\"actual len: {}\".format(len(Training_set[i]))\n",
    "    for j in range(len(Training_set[i])):\n",
    "        assert((len(Training_set[i][j])==48)),\"actual len: {}\".format(len(Training_set[i][j]))\n",
    "        for k in range(len(Training_set[i][j])):\n",
    "            assert((len(Training_set[i][j][k])==24)),\"actual len: {} {}\".format(len(Training_set[i][j][k]),k)\n",
    "                                                                                \n",
    "Training_data=np.asarray(Training_set)\n",
    "sample_num = len(Training_data)\n",
    "#Assign Label\n",
    "label = np.ones((sample_num,),dtype = int)\n",
    "label[0:100*inflation] = 0\n",
    "label[100*inflation:100*inflation+99*inflation] = 1\n",
    "label[100*inflation+99*inflation:200*inflation+99*inflation] = 2\n",
    "label[200*inflation+99*inflation:300*inflation+99*inflation] = 3\n",
    "label[300*inflation+99*inflation:400*inflation+99*inflation] = 4\n",
    "label[400*inflation+99*inflation:] = 5\n",
    "#print(Training_data.shape)\n",
    "#print(label.shape)\n",
    "train = [Training_data,label]\n",
    "train_set = np.zeros((sample_num, img_r,img_c,img_d,1))\n",
    "\n",
    "for i in range(sample_num):\n",
    "    for j in range(img_r):\n",
    "        for k in range(img_c):\n",
    "            for l in range(img_d):\n",
    "                train_set[i][j][k][l][0]=train[0][i,j,k,l]\n",
    "# for h in range(sample_num):\n",
    "#     train_set[h][:][:][:][0]=train[h]\n",
    "\n",
    "#training parameter for CNN\n",
    "classes = 6\n",
    "epoch = 25\n",
    "batch_size = 2\n",
    "#number of frames used in each video\n",
    "patch_size = 15\n",
    "\n",
    "(X_train, y_train) = (train[0],train[1])\n",
    "Y_train = np_utils.to_categorical(y_train, classes)\n",
    "\n",
    "#number of convoluntional filters\n",
    "filt =[32, # 1st layer \n",
    "       32  # 2nd layer\n",
    "      ]\n",
    "#level of pooling \n",
    "pool = [3,3]\n",
    "#level of convolution\n",
    "conv = [5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing part\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "train_set = train_set.astype('float32')\n",
    "train_set -= np.mean(train_set)\n",
    "train_set /= np.max(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNN \n",
    "In order to get a better understanding of 3D CNN. We will compare it with 2D CNN.\n",
    "\n",
    "The way of using 2D CNN to operate on video is generally to use CNN to identify each frame of the video. This method does not take the inter-frame motion information in the time dimension into account. The following is the traditional 2DCNN convolution operation on the image using 2D convolution kernel:\n",
    "![2DCNN](figure/2D.PNG)\n",
    "$$2D\\ CNN$$\n",
    "In the 2DCNN, on the convolutional layer, the 2D convolution operation extracts features in the local neighborhood of the upper level feature map. Then apply an added offset value and pass the result to a sigmoid function. Formally, at the position (x,y), on the i-th layer, and on the j-th feature map, the unit value is expressed as $v_{ij}^{xy}$, and is given by the following formula:\n",
    "$$v_{ij}^{xy}= tanh(b_{ij}+\\sum_m \\sum_{p=0}^{P_i-1} \\sum_{q=0}^{Q_i-1}w^{pq}_{ijm}v^{(x+p)(y+q)}_{(i-1)m})$$\n",
    "Where $tanh$ is a hyperbolic tangent function,$b_{ij}$ is an offset value for this feature map, m is the coordinates of an i-1 layer feature graph connected to the current feature graph set, and$w^{pq}_{ijm}$ isthe weight at the nuclear position (p, q) in mth connoected graph, $P_i$ and $Q_i$ are the height and width of the core. In the downsampling layer, on the feature map of the previous layer, the resolution of the feature map is reduced by the pooling of local neighborhoods, thereby increasing the invariance of the input distortion. A CNN architecture can be constructed by stacking multiple convolution operations and downsampling operations in an alternating manner. CNN parameters are usually trained using supervised or unsupervised methods.\n",
    "\n",
    "In the 2DCNN, the convolution operation is only applied to the 2D feature map to calculate features from the spatial dimension. When dealing with video analysis problems, it is necessary to capture the motion information encoded between consecutive frames. For this purpose, we propose that during the convolutional operation phase of CNN, a 3D convolution operation is performed to simultaneously capture features from the temporal and spatial dimensions. The 3D convolution operation is achieved by convolving a cube formed by stacking a plurality of consecutive frames simultaneously with a 3D kernel. With this construction, the feature map on the convolutional layer is connected to multiple successive frames in the previous layer, capturing the action information. Formally, at the (x, y, z) position of the i-th and j-th feature map, the value is given by the following formula:\n",
    "$$v_{ij}^{xyz}=tanh(b_{ij}+\\sum_m \\sum_{p=0}^{P_i-1} \\sum_{q=0}^{Q_i-1} \\sum_{r=0}^{R_i-1}w^{pqr}_{ijm}v_{(i-1)m}^{(x+p)(y+q)(z+r)})$$\n",
    "Where $R_i$ represents the size of the 3D core in the time dimension; $w^{pqr}_{ijm}$the value of (p,q,r) of the core of the m-th feature graph connected to the previous layer.\n",
    "\n",
    "Using 3D CNN can better capture the temporal and spatial characteristic information in the video. The following is a 3D CNN convolution operation using 3D convolution kernel for the image sequence (video):\n",
    "![3DCNN](figure/3D.PNG)\n",
    "$$3D\\ CNN$$\n",
    "The temporal dimension of the convolution operation above is 3, that is, a convolution operation is performed on consecutive three frames of images. The above 3D convolution is to form a cube by stacking a plurality of consecutive frames, and then use a 3D convolution kernel in the cube. In this structure, each feature map in the convolutional layer is connected with multiple adjacent consecutive frames in the previous layer, thus capturing motion information. For example, in the left image above, the value of a position of a convolutional map is obtained by convolving local receptive fields at the same position of three consecutive frames one level above.\n",
    "\n",
    "It should be noted that the 3D convolution kernel can only extract one type of feature from the cube because the weights of the convolution kernels are the same in the entire cube, that is, the shared weight values are all the same convolution kernel. (The same color in the figure shows the same weight). We can use a variety of convolution kernels to extract a variety of features.\n",
    "\n",
    "### Our Model\n",
    "![Model](figure/model.PNG)\n",
    "This architecture consists of 8 layers including the input. There are convolutional, rectification and sub-sampling layer each of one as C1,R1 and S1 and four neuron layers N1 to N4\n",
    "\n",
    "#### Swish activation function\n",
    "Instead of the common-used activation function ReLU, we tried to use a newly proposed activation function called ['swish'](https://arxiv.org/abs/1710.05941), which is typically $$f(x)=x\\cdot \\text{sigmoid}(x),$$ and may have ever better performance than the original ReLU. The function image of swish is shown as follows:\n",
    "![swish](figure/swish.png)\n",
    "The swish function is unbounded above and bounded below and it is the non-monotonic attribute that actually creates the difference. Here in our model, we tried to apply this activation function and see what it can help us in improving our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "#Building the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "model.add(Conv3D(\n",
    "        filters=filt[0],\n",
    "        kernel_size = (5,5,5),\n",
    "        input_shape=(img_r, img_c, img_d,1),\n",
    "        activation='swish'\n",
    "    ))\n",
    "model.add(MaxPooling3D(pool_size=(pool[0], pool[0], pool[0])))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, init='normal', activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(64,init='normal',activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(32,init='normal',activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(16,init='normal',activation='relu'))\n",
    "model.add(Dense(classes,init='normal'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['mse', 'accuracy'])\n",
    "print('Ready to Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment and Result\n",
    "In this section, we want to verify the accuracy of the 3DCNN model with KTH dataset. The size of training set is more than $2000$, for testing set is $240$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2156 samples, validate on 240 samples\n",
      "Epoch 1/25\n",
      "2156/2156 [==============================] - 31s 14ms/step - loss: 1.3705 - mean_squared_error: 0.1191 - acc: 0.3474 - val_loss: 1.1634 - val_mean_squared_error: 0.1078 - val_acc: 0.4250\n",
      "Epoch 2/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 1.1477 - mean_squared_error: 0.1029 - acc: 0.4374 - val_loss: 1.0452 - val_mean_squared_error: 0.1006 - val_acc: 0.4667\n",
      "Epoch 3/25\n",
      "2156/2156 [==============================] - 25s 12ms/step - loss: 0.9993 - mean_squared_error: 0.0942 - acc: 0.5190 - val_loss: 1.0228 - val_mean_squared_error: 0.0951 - val_acc: 0.5375\n",
      "Epoch 4/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.9105 - mean_squared_error: 0.0825 - acc: 0.6006 - val_loss: 1.0152 - val_mean_squared_error: 0.0962 - val_acc: 0.5542\n",
      "Epoch 5/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.7748 - mean_squared_error: 0.0713 - acc: 0.6707 - val_loss: 1.1863 - val_mean_squared_error: 0.0866 - val_acc: 0.6292\n",
      "Epoch 6/25\n",
      "2156/2156 [==============================] - 25s 12ms/step - loss: 0.7460 - mean_squared_error: 0.0627 - acc: 0.7073 - val_loss: 0.9125 - val_mean_squared_error: 0.0720 - val_acc: 0.6750\n",
      "Epoch 7/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6965 - mean_squared_error: 0.0567 - acc: 0.7407 - val_loss: 0.8647 - val_mean_squared_error: 0.0740 - val_acc: 0.6958\n",
      "Epoch 8/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6218 - mean_squared_error: 0.0509 - acc: 0.7709 - val_loss: 1.5395 - val_mean_squared_error: 0.0825 - val_acc: 0.6792\n",
      "Epoch 9/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6199 - mean_squared_error: 0.0478 - acc: 0.7839 - val_loss: 1.1025 - val_mean_squared_error: 0.0766 - val_acc: 0.6833\n",
      "Epoch 10/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6415 - mean_squared_error: 0.0452 - acc: 0.8010 - val_loss: 1.2019 - val_mean_squared_error: 0.0803 - val_acc: 0.6583\n",
      "Epoch 11/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6323 - mean_squared_error: 0.0431 - acc: 0.8135 - val_loss: 1.1520 - val_mean_squared_error: 0.0657 - val_acc: 0.7000\n",
      "Epoch 12/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6363 - mean_squared_error: 0.0417 - acc: 0.8210 - val_loss: 1.3351 - val_mean_squared_error: 0.0783 - val_acc: 0.6750\n",
      "Epoch 13/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.5700 - mean_squared_error: 0.0403 - acc: 0.8228 - val_loss: 1.3652 - val_mean_squared_error: 0.0729 - val_acc: 0.6708\n",
      "Epoch 14/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6297 - mean_squared_error: 0.0390 - acc: 0.8270 - val_loss: 1.6205 - val_mean_squared_error: 0.0720 - val_acc: 0.7125\n",
      "Epoch 15/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.6595 - mean_squared_error: 0.0394 - acc: 0.8363 - val_loss: 1.2837 - val_mean_squared_error: 0.0686 - val_acc: 0.7042\n",
      "Epoch 16/25\n",
      "2156/2156 [==============================] - 26s 12ms/step - loss: 0.5867 - mean_squared_error: 0.0341 - acc: 0.8595 - val_loss: 2.4777 - val_mean_squared_error: 0.0883 - val_acc: 0.6667\n",
      "Epoch 17/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.6213 - mean_squared_error: 0.0357 - acc: 0.8497 - val_loss: 2.1366 - val_mean_squared_error: 0.0922 - val_acc: 0.6667\n",
      "Epoch 18/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.8198 - mean_squared_error: 0.0399 - acc: 0.8423 - val_loss: 2.3301 - val_mean_squared_error: 0.0805 - val_acc: 0.7167\n",
      "Epoch 19/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.8140 - mean_squared_error: 0.0397 - acc: 0.8298 - val_loss: 2.0108 - val_mean_squared_error: 0.0842 - val_acc: 0.6667\n",
      "Epoch 20/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.7660 - mean_squared_error: 0.0385 - acc: 0.8404 - val_loss: 2.3947 - val_mean_squared_error: 0.0791 - val_acc: 0.7167\n",
      "Epoch 21/25\n",
      "2156/2156 [==============================] - 25s 11ms/step - loss: 0.8847 - mean_squared_error: 0.0368 - acc: 0.8525 - val_loss: 2.5086 - val_mean_squared_error: 0.0822 - val_acc: 0.6917\n",
      "Epoch 22/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.8255 - mean_squared_error: 0.0364 - acc: 0.8562 - val_loss: 2.2833 - val_mean_squared_error: 0.0797 - val_acc: 0.6917\n",
      "Epoch 23/25\n",
      "2156/2156 [==============================] - 24s 11ms/step - loss: 0.6376 - mean_squared_error: 0.0320 - acc: 0.8673 - val_loss: 2.2420 - val_mean_squared_error: 0.0736 - val_acc: 0.7417\n",
      "Epoch 24/25\n",
      "2156/2156 [==============================] - 25s 11ms/step - loss: 0.9396 - mean_squared_error: 0.0413 - acc: 0.8270 - val_loss: 3.0096 - val_mean_squared_error: 0.0853 - val_acc: 0.6875\n",
      "Epoch 25/25\n",
      "2156/2156 [==============================] - 25s 11ms/step - loss: 0.9334 - mean_squared_error: 0.0378 - acc: 0.8511 - val_loss: 2.5860 - val_mean_squared_error: 0.0821 - val_acc: 0.6917\n",
      "240/240 [==============================] - 1s 3ms/step\n",
      "Test score: [2.586045242365005, 0.08206570617768945, 0.6916666666666667]\n"
     ]
    }
   ],
   "source": [
    "#print(Y_train.shape)\n",
    "#Split the data for Train and Test\n",
    "X_train_new, X_val_new, y_train_new,y_val_new = train_test_split(train_set, Y_train, test_size=0.1, random_state=4)\n",
    "#print(X_train_new.shape)\n",
    "#print(y_train_new.shape)\n",
    "#Training\n",
    "hist = model.fit(\n",
    "    X_train_new,\n",
    "    y_train_new,\n",
    "    validation_data=(X_val_new,y_val_new),\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch = epoch,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "#Testing\n",
    "score = model.evaluate(\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    batch_size=batch_size,\n",
    "    #show_accuracy=True\n",
    "    )\n",
    "\n",
    "#print(model.metrics_names);\n",
    "\n",
    "print('Test score:', score)\n",
    "\n",
    "#print('History', hist.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that after 25 rounds, the accuracy is near 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we adopted the 3D CNN network to do classification of human activities and tried a new activation function which is newly proposed to verify that our model can actually work. As a result, the trainning accuracy can reach $85\\%$ and the verifying accuracy can reach $70\\%$, which means our model can do the classification and have resonable outcome. However, this model can also have increased accuracy and may need to do some modification of the model structure such as increasing the neural network depth and we can also add the size of training dataset to make our model better classify human actions. Other work can be done in the future include better selecting the training dataset and extend our model to mutiple channel frames which could provide more information for the model to do classify work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division of labor\n",
    "#### Yu Mi:\n",
    "+ Moving tensorflow from CPU to GPU\n",
    "+ Finishing division of training dataset\n",
    "+ Implementing and applying swish function\n",
    "+ Training and modifying model\n",
    "#### Boning Zhao:\n",
    "+ Importing training dataset\n",
    "+ Building initial model\n",
    "+ Developing trainning and testing work flow\n",
    "+ Text editing work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
