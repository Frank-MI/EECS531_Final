{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Author: Yu Mi, yxm319; Boning Zhao, bxz213\n",
    "Recognizing human actions is one of most popular computer vision method which finds mutiple applications in lots of fields such as video surveillance, customer attributes, shopping behavior analysis.\n",
    "\n",
    "In our final project, we consider the automated recognition of human actions in some videos. We proposed to build up a 3D CNN model for action recognition. In order to capture motion information from multiple adjacent frames, we proposed to extract features from both spatial and temporal dimensions. Based on this feature extractor, a 3D convolutional neural network will be built up. This CNN will generates multiple channels of information and performs convolution and subsampling separately. The final feature representation is obtained by conbining information from all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Import standard and supportive libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Available devices for trainning:\", get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nerual network framework\n",
    "In this project, we are going to apply [Keras](https://keras.io) as our neural network framework since it is already introduced in Homework3. It is capable of running on top of TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay and good for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Try to use tensorflow in GPU\n",
    "config_tf = tf.ConfigProto(log_device_placement=True)\n",
    "config_tf.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config_tf)\n",
    "\n",
    "# Import models and layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "\n",
    "# Import utilities\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.backend import set_session\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KTH dataset\n",
    "[KTH dataset](http://www.nada.kth.se/cvap/actions/) is a database provided by KTH Royal institute of Technology. The current video database contains six tyes of human actions, including walking, jogging, running, boxing, hand waving and hand clapping. All the actions are performed several times by 25 different individuals in for scenarios: outdoors $s1$, outdoors with scale variation $s2$, outdoors with different clothes $s3$ and indoors $s4$ as illustrated below. \n",
    "![KTH scenarios and actions](figure/KTH_Intro.gif)\n",
    "Currently we have $600$ sequences in the dataset and all the sequences were taken over homogeneous backgrounds with a static camera with $25$fps frame rate. The sequences were downsampled to the spatial resolution of $160\\times120$ pixels and have a length of four seconds in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxing class has been loaded\n",
      "Hand clapping class has been loaded\n",
      "Hand waving class has been loaded\n",
      "Jogging class has been loaded\n",
      "running class has been loaded\n",
      "walking class has been loaded\n"
     ]
    }
   ],
   "source": [
    "# image attributes\n",
    "img_r, img_c, img_d = 34, 54, 9\n",
    "#img_r, img_c, img_d = 15, 15, 16\n",
    "#Training set\n",
    "#Entire dataset\n",
    "Training_set=[]\n",
    "#Loading boxing class\n",
    "box_listing = os.listdir('data/kth_database/boxing')\n",
    "for box_id in box_listing:\n",
    "    box_id = 'data/kth_database/boxing/'+box_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(box_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Boxing class has been loaded\")  \n",
    "\n",
    "#Loading hand clapping class\n",
    "hc_listing = os.listdir('data/kth_database/handclapping')\n",
    "for hc_id in hc_listing:\n",
    "    hc_id = 'data/kth_database/handclapping/'+hc_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hc_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand clapping class has been loaded\")\n",
    "\n",
    "#Loading hand waving class\n",
    "hw_listing = os.listdir('data/kth_database/handwaving')\n",
    "for hw_id in hw_listing:\n",
    "    hw_id = 'data/kth_database/handwaving/'+hw_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(hw_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Hand waving class has been loaded\")\n",
    "\n",
    "#Loading jogging class\n",
    "jog_listing = os.listdir('data/kth_database/jogging')\n",
    "for jog_id in jog_listing:\n",
    "    jog_id = 'data/kth_database/jogging/'+jog_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(jog_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"Jogging class has been loaded\")\n",
    "\n",
    "#Loading running class\n",
    "run_listing = os.listdir('data/kth_database/running')\n",
    "for run_id in run_listing:\n",
    "    run_id = 'data/kth_database/running/'+run_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(run_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"running class has been loaded\")\n",
    "\n",
    "#Loading walking class\n",
    "walk_listing = os.listdir('data/kth_database/walking')\n",
    "for walk_id in walk_listing:\n",
    "    walk_id = 'data/kth_database/walking/'+walk_id\n",
    "    frame_list = []\n",
    "    capture = cv2.VideoCapture(walk_id)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    #print(\"Frames per second using video.get(cv2.CAP_PROP_FPS): {0}\".format(fps))\n",
    "    for i in range(img_d):\n",
    "        success, frame = capture.read()\n",
    "        frame = cv2.resize(frame,(img_r,img_c),interpolation=cv2.INTER_AREA)\n",
    "        gray =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_list.append(gray)\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        # to hide tick values on X and Y axis\n",
    "        #plt.xticks([]), plt.yticks([])\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "        #Indicates the number of milliseconds to wait. It will wait for a specific number of milliseconds to see if the keyboard has any input. The return value is ASCII. If its parameter is 0, it means indefinitely waiting for keyboard input\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ipt = np.asarray(frame_list)\n",
    "    ipt=np.rollaxis(np.rollaxis(ipt,2,0),2,1)\n",
    "    Training_set.append(ipt)\n",
    "print(\"walking class has been loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 34, 54, 9)\n",
      "(599,)\n",
      "Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:96: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "#convert the fram into array\n",
    "Training_data=np.array(Training_set)\n",
    "sample_num = len(Training_data)\n",
    "#Assign Label\n",
    "label = np.ones((sample_num,),dtype = int)\n",
    "label[0:100] = 0\n",
    "label[100:199] = 1\n",
    "label[199:299] = 2\n",
    "label[299:399] = 3\n",
    "label[399:499] = 4\n",
    "label[499:] = 5\n",
    "print(Training_data.shape)\n",
    "print(label.shape)\n",
    "train= [Training_data,label]\n",
    "train_set = np.zeros((sample_num, img_r,img_c,img_d,1))\n",
    "for i in range(sample_num):\n",
    "    for j in range(img_r):\n",
    "        for k in range(img_c):\n",
    "            for l in range(img_d):\n",
    "                train_set[i][j][k][l][0]=train[0][i,j,k,l]\n",
    "                #print(i)\n",
    "#training parameter for CNN\n",
    "classes = 6\n",
    "epoch =50\n",
    "batch_size = 2\n",
    "#number of frames\n",
    "patch_size = 15\n",
    "\n",
    "\n",
    "(X_train, y_train) = (train[0],train[1])\n",
    "Y_train = np_utils.to_categorical(y_train, classes)\n",
    "\n",
    "#number of convoluntional filters\n",
    "filt =[32, # 1st latyer \n",
    "       32  # 2nd layer\n",
    "      ]\n",
    "#level of pooling \n",
    "pool = [3,3]\n",
    "#level of convolution\n",
    "conv = [5,5]\n",
    "\n",
    "#preprocessing part\n",
    "train_set = train_set.astype('float32')\n",
    "train_set -= np.mean(train_set)\n",
    "train_set /= np.max(train_set)\n",
    "\n",
    "#Building the CNN model\n",
    "\n",
    "model = Sequential()\n",
    "'''\n",
    "model.add(Conv3D(\n",
    "        filters=7,\n",
    "        kernel_size = (28,48,5),\n",
    "        strides=(1, 1, 1),\n",
    "        activation='relu',\n",
    "        input_shape = (34,54,9,7)\n",
    "        ))\n",
    "\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(14, 24, 5)))\n",
    "print('Test')\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Conv3D(\n",
    "        filters=35,\n",
    "        kernel_size = (10,20,3),\n",
    "        strides=(1, 1, 1),\n",
    "        activation='relu',\n",
    "        ))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(5, 10, 3)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Conv3D(\n",
    "        filters=5,\n",
    "        kernel_size = (3,8,1),\n",
    "        strides=(1, 1, 1),\n",
    "        activation='relu',\n",
    "        ))\n",
    "model.add(Dense(30,init='normal'))\n",
    "model.add(Dense(6,init='normal'))\n",
    "'''\n",
    "model.add(Conv3D(\n",
    "        filters=filt[0],\n",
    "        kernel_size = (5,5,5),\n",
    "        input_shape=(img_r, img_c, img_d,1),\n",
    "        activation='relu'\n",
    "    ))\n",
    "print('Test')\n",
    "model.add(MaxPooling3D(pool_size=(pool[0], pool[0], pool[0])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, init='normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes,init='normal'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['mse', 'accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 6)\n",
      "(479, 34, 54, 9, 1)\n",
      "(479, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 479 samples, validate on 120 samples\n",
      "Epoch 1/50\n",
      "479/479 [==============================] - 10s 20ms/step - loss: 1.6806 - mean_squared_error: 0.1352 - acc: 0.2526 - val_loss: 1.3227 - val_mean_squared_error: 0.1154 - val_acc: 0.4417\n",
      "Epoch 2/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.4203 - mean_squared_error: 0.1251 - acc: 0.3737 - val_loss: 1.1164 - val_mean_squared_error: 0.1028 - val_acc: 0.4417\n",
      "Epoch 3/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 1.3250 - mean_squared_error: 0.1176 - acc: 0.4050 - val_loss: 1.1335 - val_mean_squared_error: 0.1084 - val_acc: 0.4667\n",
      "Epoch 4/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.2731 - mean_squared_error: 0.1143 - acc: 0.4280 - val_loss: 0.9674 - val_mean_squared_error: 0.0921 - val_acc: 0.5083\n",
      "Epoch 5/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.2043 - mean_squared_error: 0.1117 - acc: 0.4426 - val_loss: 1.0219 - val_mean_squared_error: 0.1027 - val_acc: 0.4417\n",
      "Epoch 6/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.1901 - mean_squared_error: 0.1080 - acc: 0.4948 - val_loss: 0.9054 - val_mean_squared_error: 0.0893 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.0945 - mean_squared_error: 0.0998 - acc: 0.5157 - val_loss: 0.9579 - val_mean_squared_error: 0.0909 - val_acc: 0.5750\n",
      "Epoch 8/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.0380 - mean_squared_error: 0.0972 - acc: 0.5344 - val_loss: 0.8120 - val_mean_squared_error: 0.0777 - val_acc: 0.6000\n",
      "Epoch 9/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.0143 - mean_squared_error: 0.0923 - acc: 0.5658 - val_loss: 0.8181 - val_mean_squared_error: 0.0799 - val_acc: 0.6083\n",
      "Epoch 10/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.9883 - mean_squared_error: 0.0886 - acc: 0.5762 - val_loss: 0.8388 - val_mean_squared_error: 0.0780 - val_acc: 0.6167\n",
      "Epoch 11/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.0668 - mean_squared_error: 0.0874 - acc: 0.6159 - val_loss: 0.8365 - val_mean_squared_error: 0.0761 - val_acc: 0.6667\n",
      "Epoch 12/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.9805 - mean_squared_error: 0.0846 - acc: 0.6054 - val_loss: 1.0894 - val_mean_squared_error: 0.0944 - val_acc: 0.5750\n",
      "Epoch 13/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 0.9137 - mean_squared_error: 0.0803 - acc: 0.6430 - val_loss: 0.8139 - val_mean_squared_error: 0.0724 - val_acc: 0.6500\n",
      "Epoch 14/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8953 - mean_squared_error: 0.0777 - acc: 0.6326 - val_loss: 0.9033 - val_mean_squared_error: 0.0803 - val_acc: 0.6000\n",
      "Epoch 15/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.9339 - mean_squared_error: 0.0819 - acc: 0.6180 - val_loss: 0.9047 - val_mean_squared_error: 0.0825 - val_acc: 0.6167\n",
      "Epoch 16/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 0.9570 - mean_squared_error: 0.0776 - acc: 0.6514 - val_loss: 0.8965 - val_mean_squared_error: 0.0813 - val_acc: 0.6083\n",
      "Epoch 17/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8821 - mean_squared_error: 0.0753 - acc: 0.6660 - val_loss: 1.0217 - val_mean_squared_error: 0.0872 - val_acc: 0.6000\n",
      "Epoch 18/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8934 - mean_squared_error: 0.0742 - acc: 0.6743 - val_loss: 0.9093 - val_mean_squared_error: 0.0758 - val_acc: 0.6500\n",
      "Epoch 19/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8643 - mean_squared_error: 0.0735 - acc: 0.6701 - val_loss: 0.9357 - val_mean_squared_error: 0.0792 - val_acc: 0.6083\n",
      "Epoch 20/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8219 - mean_squared_error: 0.0691 - acc: 0.7056 - val_loss: 1.1121 - val_mean_squared_error: 0.0916 - val_acc: 0.6167\n",
      "Epoch 21/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8317 - mean_squared_error: 0.0687 - acc: 0.7056 - val_loss: 0.9970 - val_mean_squared_error: 0.0799 - val_acc: 0.6333\n",
      "Epoch 22/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8591 - mean_squared_error: 0.0721 - acc: 0.6785 - val_loss: 1.0705 - val_mean_squared_error: 0.0862 - val_acc: 0.5833\n",
      "Epoch 23/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8151 - mean_squared_error: 0.0648 - acc: 0.7328 - val_loss: 0.9624 - val_mean_squared_error: 0.0754 - val_acc: 0.6500\n",
      "Epoch 24/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8719 - mean_squared_error: 0.0733 - acc: 0.6827 - val_loss: 1.0369 - val_mean_squared_error: 0.0829 - val_acc: 0.6167\n",
      "Epoch 25/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8163 - mean_squared_error: 0.0643 - acc: 0.7286 - val_loss: 0.9932 - val_mean_squared_error: 0.0807 - val_acc: 0.6417\n",
      "Epoch 26/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8166 - mean_squared_error: 0.0682 - acc: 0.7015 - val_loss: 1.0537 - val_mean_squared_error: 0.0844 - val_acc: 0.6000\n",
      "Epoch 27/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8018 - mean_squared_error: 0.0681 - acc: 0.7035 - val_loss: 0.9446 - val_mean_squared_error: 0.0747 - val_acc: 0.6500\n",
      "Epoch 28/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7976 - mean_squared_error: 0.0671 - acc: 0.7098 - val_loss: 1.0503 - val_mean_squared_error: 0.0872 - val_acc: 0.5917\n",
      "Epoch 29/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8316 - mean_squared_error: 0.0689 - acc: 0.7077 - val_loss: 0.9734 - val_mean_squared_error: 0.0826 - val_acc: 0.5917\n",
      "Epoch 30/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7862 - mean_squared_error: 0.0665 - acc: 0.6973 - val_loss: 1.1259 - val_mean_squared_error: 0.0876 - val_acc: 0.6083\n",
      "Epoch 31/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6926 - mean_squared_error: 0.0639 - acc: 0.7035 - val_loss: 0.9719 - val_mean_squared_error: 0.0782 - val_acc: 0.5833\n",
      "Epoch 32/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7231 - mean_squared_error: 0.0609 - acc: 0.7203 - val_loss: 1.0829 - val_mean_squared_error: 0.0840 - val_acc: 0.6167\n",
      "Epoch 33/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8123 - mean_squared_error: 0.0665 - acc: 0.7119 - val_loss: 1.0761 - val_mean_squared_error: 0.0841 - val_acc: 0.6250\n",
      "Epoch 34/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6841 - mean_squared_error: 0.0584 - acc: 0.7453 - val_loss: 1.1138 - val_mean_squared_error: 0.0772 - val_acc: 0.6583\n",
      "Epoch 35/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7583 - mean_squared_error: 0.0616 - acc: 0.7286 - val_loss: 1.0683 - val_mean_squared_error: 0.0815 - val_acc: 0.6583\n",
      "Epoch 36/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7135 - mean_squared_error: 0.0589 - acc: 0.7516 - val_loss: 1.3632 - val_mean_squared_error: 0.0902 - val_acc: 0.6083\n",
      "Epoch 37/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8244 - mean_squared_error: 0.0640 - acc: 0.7411 - val_loss: 1.0571 - val_mean_squared_error: 0.0864 - val_acc: 0.5917\n",
      "Epoch 38/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 0.8181 - mean_squared_error: 0.0548 - acc: 0.7578 - val_loss: 1.2086 - val_mean_squared_error: 0.0857 - val_acc: 0.6250\n",
      "Epoch 39/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7247 - mean_squared_error: 0.0614 - acc: 0.7328 - val_loss: 1.1921 - val_mean_squared_error: 0.0762 - val_acc: 0.6667\n",
      "Epoch 40/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6804 - mean_squared_error: 0.0582 - acc: 0.7641 - val_loss: 1.1811 - val_mean_squared_error: 0.0839 - val_acc: 0.6333\n",
      "Epoch 41/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6539 - mean_squared_error: 0.0542 - acc: 0.7704 - val_loss: 1.0630 - val_mean_squared_error: 0.0787 - val_acc: 0.6333\n",
      "Epoch 42/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8040 - mean_squared_error: 0.0579 - acc: 0.7662 - val_loss: 1.1629 - val_mean_squared_error: 0.0822 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6132 - mean_squared_error: 0.0516 - acc: 0.7912 - val_loss: 1.2446 - val_mean_squared_error: 0.0829 - val_acc: 0.6333\n",
      "Epoch 44/50\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 0.8126 - mean_squared_error: 0.0564 - acc: 0.7724 - val_loss: 1.1906 - val_mean_squared_error: 0.0776 - val_acc: 0.6667\n",
      "Epoch 45/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7449 - mean_squared_error: 0.0547 - acc: 0.7683 - val_loss: 1.2686 - val_mean_squared_error: 0.0813 - val_acc: 0.6333\n",
      "Epoch 46/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.8003 - mean_squared_error: 0.0558 - acc: 0.7829 - val_loss: 1.3194 - val_mean_squared_error: 0.0850 - val_acc: 0.6333\n",
      "Epoch 47/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7279 - mean_squared_error: 0.0562 - acc: 0.7641 - val_loss: 1.1837 - val_mean_squared_error: 0.0754 - val_acc: 0.6667\n",
      "Epoch 48/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6896 - mean_squared_error: 0.0529 - acc: 0.7829 - val_loss: 1.2445 - val_mean_squared_error: 0.0796 - val_acc: 0.6250\n",
      "Epoch 49/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.6817 - mean_squared_error: 0.0510 - acc: 0.7975 - val_loss: 1.3880 - val_mean_squared_error: 0.0875 - val_acc: 0.6333\n",
      "Epoch 50/50\n",
      "479/479 [==============================] - 2s 5ms/step - loss: 0.7223 - mean_squared_error: 0.0548 - acc: 0.7724 - val_loss: 1.2647 - val_mean_squared_error: 0.0842 - val_acc: 0.6167\n",
      "120/120 [==============================] - 0s 1ms/step\n",
      "Test score: [1.2646580506310168, 0.08417850999112403, 0.6166666666666667]\n",
      "History {'val_loss': [1.3227023671070735, 1.1163533051808676, 1.1335030357042948, 0.9673980002601942, 1.021909582770119, 0.9053889616082113, 0.9579182216897607, 0.81200244029363, 0.818149478174746, 0.8388350069678078, 0.836540861458828, 1.08939384262776, 0.8139498151190007, 0.9033369694376991, 0.9047340942289641, 0.8964599703304127, 1.0217194020409806, 0.9092896478672173, 0.9356685513740179, 1.1121024678143536, 0.9969950439313948, 1.0704952314312055, 0.9624201439139294, 1.0368735405132463, 0.9932360670328403, 1.0536805268641427, 0.9445711381835589, 1.0502850565738604, 0.9734423320961888, 1.1258924667490193, 0.9719235978575557, 1.0829120325013493, 1.0761024689820415, 1.113789667098093, 1.0683020387157747, 1.3631801804327035, 1.0570668945642576, 1.208605256548193, 1.1920650033052536, 1.181145314398938, 1.0629637453114513, 1.1629144007649157, 1.24460710948282, 1.1905795925468756, 1.2686096771015636, 1.3193645124367175, 1.1836676718827512, 1.2445219665595244, 1.3879862968697643, 1.2646580506310168], 'val_mean_squared_error': [0.11535129602998495, 0.1028117010059456, 0.10838299548874299, 0.09209014101264378, 0.10265196736824388, 0.08926257218117825, 0.09088984957003655, 0.07770404708959783, 0.07994397453197356, 0.07795378468661956, 0.07614037596721725, 0.09444206316884732, 0.07235357765806988, 0.0802899522462294, 0.08248028322195082, 0.08126221227422743, 0.08719503866758978, 0.07575347957371943, 0.07916808659497734, 0.09156806661109532, 0.07987293659976574, 0.08618274601710835, 0.07540428668228889, 0.08292868840936993, 0.08073016226115773, 0.08443325245740727, 0.07465569353801804, 0.08723254409452122, 0.08263755053026063, 0.08757203597340661, 0.07815888333806553, 0.08400267577260756, 0.084055468793069, 0.07715627603492509, 0.08150517523229421, 0.09024957455281403, 0.08644779023848947, 0.08568969274909301, 0.0762270232317865, 0.08385926004253198, 0.07871629149787987, 0.08221967566336381, 0.08290905085409145, 0.07760049367511797, 0.08127330271444293, 0.08499744826839514, 0.07537063264115448, 0.07958300541388065, 0.0874928139980358, 0.08417850999112403], 'val_acc': [0.44166666666666665, 0.44166666666666665, 0.4666666666666667, 0.5083333333333333, 0.44166666666666665, 0.5, 0.575, 0.6, 0.6083333333333333, 0.6166666666666667, 0.6666666666666666, 0.575, 0.65, 0.6, 0.6166666666666667, 0.6083333333333333, 0.6, 0.65, 0.6083333333333333, 0.6166666666666667, 0.6333333333333333, 0.5833333333333334, 0.65, 0.6166666666666667, 0.6416666666666667, 0.6, 0.65, 0.5916666666666667, 0.5916666666666667, 0.6083333333333333, 0.5833333333333334, 0.6166666666666667, 0.625, 0.6583333333333333, 0.6583333333333333, 0.6083333333333333, 0.5916666666666667, 0.625, 0.6666666666666666, 0.6333333333333333, 0.6333333333333333, 0.65, 0.6333333333333333, 0.6666666666666666, 0.6333333333333333, 0.6333333333333333, 0.6666666666666666, 0.625, 0.6333333333333333, 0.6166666666666667], 'loss': [1.680586088549867, 1.420341067373877, 1.3249954589971171, 1.2731292813954125, 1.2042532955959346, 1.1900723532682171, 1.0945369202414206, 1.0380333523929026, 1.0142739101241116, 0.9882986074506007, 1.0668088432509055, 0.9805439470314211, 0.9136892639548106, 0.8952929623288224, 0.9339256868748665, 0.9569591707312495, 0.882134560321875, 0.8934458332637186, 0.8642787990640062, 0.8219292962756162, 0.8317245679111205, 0.8591409267716134, 0.8150778643659226, 0.87186205500076, 0.8163494843922675, 0.8166327426643459, 0.801829136672389, 0.7975804841296714, 0.8315892241326254, 0.7861925479969468, 0.6926217693695831, 0.7231303856739618, 0.8122601068024213, 0.6840789229668771, 0.7583432219407487, 0.7135195770051062, 0.8244060933357632, 0.8180508509635667, 0.7246846394667706, 0.6803764593242633, 0.6538904059155488, 0.804021741060041, 0.6131886976004243, 0.8126322044992742, 0.7448975729578994, 0.8003049434397606, 0.7278584257963958, 0.6895734742054971, 0.6817155247776577, 0.7222726744589805], 'mean_squared_error': [0.13515386560889028, 0.12514739069846578, 0.11761723456044909, 0.11433011377794135, 0.11171173308062171, 0.10795476140130807, 0.09977139429637473, 0.09720768530989672, 0.09229255556645527, 0.08858857588963032, 0.08743251964775312, 0.08464009544713683, 0.08033377552590736, 0.07771873454729628, 0.08194231398987115, 0.07759016660253064, 0.07533821368106292, 0.07422296937429212, 0.07346770501010032, 0.06905934468746017, 0.06865329420097091, 0.07213469752360892, 0.06483754261241395, 0.0732704912475852, 0.06425147428256199, 0.0681697761729416, 0.06814135273176805, 0.06712251253057619, 0.06894093099777061, 0.06646265387441604, 0.06390952823522171, 0.06092420712714593, 0.06648471635107447, 0.058409756100042, 0.0615892478169352, 0.0589152634571731, 0.06403766964801136, 0.05479084092573288, 0.061401333807913856, 0.058201326282009476, 0.05422071029842329, 0.05787748517405092, 0.05159506848832559, 0.05639525309459934, 0.05468279455493445, 0.05583136177464408, 0.056155672903799816, 0.0528880517961425, 0.051022587053118434, 0.054812406554540095], 'acc': [0.25260960334029225, 0.3736951983298539, 0.40501043841336115, 0.4279749478079332, 0.44258872651356995, 0.49478079331941544, 0.5156576200417536, 0.534446764091858, 0.5657620041753654, 0.5762004175365344, 0.615866388308977, 0.605427974947808, 0.6430062630480167, 0.6325678496868476, 0.6179540709812108, 0.651356993736952, 0.6659707724425887, 0.6743215031315241, 0.6701461377870563, 0.7056367432150313, 0.7056367432150313, 0.6784968684759917, 0.732776617954071, 0.6826722338204593, 0.7286012526096033, 0.7014613778705637, 0.7035490605427975, 0.7098121085594989, 0.7077244258872651, 0.697286012526096, 0.7035490605427975, 0.7202505219206681, 0.7118997912317327, 0.7453027139874739, 0.7286012526096033, 0.7515657620041754, 0.7411273486430062, 0.7578288100208769, 0.732776617954071, 0.7640918580375783, 0.7703549060542797, 0.7661795407098121, 0.791231732776618, 0.7724425887265136, 0.7682672233820459, 0.7828810020876826, 0.7640918580375783, 0.7828810020876826, 0.7974947807933194, 0.7724425887265136]}\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "#Split the data for Train and Test\n",
    "X_train_new, X_val_new, y_train_new,y_val_new = train_test_split(train_set, Y_train, test_size=0.2, random_state=4)\n",
    "print(X_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "#Training\n",
    "hist = model.fit(\n",
    "    X_train_new,\n",
    "    y_train_new,\n",
    "    validation_data=(X_val_new,y_val_new),\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch = epoch,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "#Testing\n",
    "score = model.evaluate(\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    batch_size=batch_size,\n",
    "    #show_accuracy=True\n",
    "    )\n",
    "\n",
    "print('Test score:', score)\n",
    "\n",
    "print('History', hist.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.rollaxis(ipt,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.rollaxis(np.rollaxis(ipt,2,0),2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
